{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import bambi as bmb\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import sidetable\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведем симуляцию на основе toy problem из библиотеки space bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space_bandits.toy_problem import get_customer_nl, get_rewards_nl, get_cust_reward_nl\n",
    "\n",
    "get_customer = get_customer_nl\n",
    "get_rewards = get_rewards_nl\n",
    "get_cust_reward = get_cust_reward_nl\n",
    "optimal_choices = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем записывать результаты каждого подохода к определению лучшей стратегии для минимизации потерть (regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cmab_regrets = pd.DataFrame()\n",
    "df_cmab_rewards = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рандомно выберем ручки для пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_regret_list= []\n",
    "cumulative_reward_list = []\n",
    "total_regret = 0\n",
    "cumulative_reward = 0\n",
    "n_arms = 3\n",
    "arm_indices = np.array(range(n_arms))\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(desc = 'while loop', total = n)\n",
    "while i != n:\n",
    "    \n",
    "    #generate a customer\n",
    "    cust = get_customer()\n",
    "    reward_vec = get_rewards(cust)\n",
    "                    \n",
    "        # Display the selected arm\n",
    "    i += 1\n",
    "    arm = np.random.choice(arm_indices)\n",
    "    \n",
    "    reward = reward_vec[arm]\n",
    "    \n",
    "    best_choice = optimal_choices[cust[0]]\n",
    "    mx = reward_vec[best_choice]\n",
    "    \n",
    "    regret = mx - reward\n",
    "    total_regret += regret\n",
    "    cumulative_regret_list.append(total_regret)\n",
    "    cumulative_reward += reward\n",
    "    cumulative_reward_list.append(cumulative_reward)\n",
    "    pbar.update(1)\n",
    "            \n",
    "pbar.close()\n",
    "df_cmab_regrets['Random'] = cumulative_regret_list\n",
    "df_cmab_rewards['Random'] = cumulative_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся MAB алгоритмом UCB, который не учитывает контекст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_regret_list= []\n",
    "cumulative_reward_list = []\n",
    "total_regret = 0\n",
    "cumulative_reward = 0\n",
    "n_arms = 3\n",
    "arm_indices = np.array(range(n_arms))\n",
    "i = 0\n",
    "coef = 100\n",
    "Q = np.zeros(n_arms)\n",
    "uncertainty = np.zeros(n_arms)\n",
    "N = np.zeros(n_arms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(desc = 'while loop', total = n)\n",
    "while i != n:\n",
    "    \n",
    "    #generate a customer\n",
    "    cust = get_customer()\n",
    "    reward_vec = get_rewards(cust)\n",
    "    context = np.array([cust[1]])\n",
    "        \n",
    "                    \n",
    "        # Display the selected arm\n",
    "    if any(N==0):\n",
    "        arm = np.random.choice(arm_indices[N==0])\n",
    "    else:\n",
    "        uncertainty = np.sqrt(np.log(i) / N)\n",
    "        arm = np.argmax(Q +  coef * uncertainty)\n",
    "    i += 1\n",
    "    \n",
    "    reward = reward_vec[arm]\n",
    "    \n",
    "    N[arm] += 1\n",
    "    Q[arm] += (1 / N[arm]) * (reward - Q[arm])\n",
    "    \n",
    "    best_choice = optimal_choices[cust[0]]\n",
    "    mx = reward_vec[best_choice]\n",
    "\n",
    "    regret = mx - reward\n",
    "    total_regret += regret\n",
    "    cumulative_regret_list.append(total_regret)\n",
    "    cumulative_reward += reward\n",
    "    cumulative_reward_list.append(cumulative_reward)\n",
    "    pbar.update(1)\n",
    "            \n",
    "pbar.close()\n",
    "df_cmab_regrets['UCB'] = cumulative_regret_list\n",
    "df_cmab_rewards['UCB'] = cumulative_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spance bandits linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся готовой реализацией линейного CMAB на сэмплировании Томпсона "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space_bandits import LinearBandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 3 \n",
    "num_features = 2\n",
    "model = LinearBandits(num_actions, num_features)\n",
    "X = []\n",
    "y = []\n",
    "a = []\n",
    "total_regret = 0\n",
    "cumulative_reward = 0\n",
    "cumulative_regret_list= []\n",
    "cumulative_reward_list = []\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(desc = 'while loop', total = n)\n",
    "while i != n:\n",
    "    \n",
    "    #generate a customer\n",
    "    cust = get_customer()\n",
    "    reward_vec = get_rewards(cust)\n",
    "    #prepare features for model\n",
    "    context = np.array([cust[1]])\n",
    "        \n",
    "                    \n",
    "        # Display the selected arm\n",
    "    if (i+1) < 500:\n",
    "        arm = np.random.choice(arm_indices)\n",
    "    else:\n",
    "        arm = model.action(context)\n",
    "    i += 1\n",
    "    \n",
    "    reward = reward_vec[arm]\n",
    "    X.append(context.tolist()[0])\n",
    "    y.append(reward)\n",
    "    a.append(arm)\n",
    "    \n",
    "    best_choice = optimal_choices[cust[0]]\n",
    "    mx = reward_vec[best_choice]\n",
    "    regret = mx - reward\n",
    "    total_regret += regret\n",
    "    cumulative_regret_list.append(total_regret)\n",
    "    cumulative_reward += reward\n",
    "    cumulative_reward_list.append(cumulative_reward)\n",
    "    pbar.update(1)\n",
    "    #Update the models with the latest batch of data\n",
    "    if i  % 500 == 0 and i  != n :\n",
    "        print(\"Updating the models at i:\", i )\n",
    "        \n",
    "        train_df = pd.DataFrame(X,\n",
    "                            columns=[\"Age\", 'ARPU'])\n",
    "        train_df['reward'] = y\n",
    "        train_df['arm'] = a\n",
    "        print('Распределение ручек:')\n",
    "        print(train_df.arm.value_counts())\n",
    "        print(f\"Текущий regret: {cumulative_regret_list[-1]}\")\n",
    "        \n",
    "        for index, row in tqdm(train_df.iterrows(),total =train_df.shape[0]):\n",
    "            context = row[[\"Age\", 'ARPU']].values\n",
    "            action = int(row['arm'])\n",
    "            reward = float(row['reward'])\n",
    "            model.update(context, action, reward)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        a = []\n",
    "            \n",
    "pbar.close()\n",
    "df_cmab_regrets['SB_linear'] = cumulative_regret_list\n",
    "df_cmab_rewards['SB_linear'] = cumulative_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space bandits Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся готовой реализацией нелинейного CMAB на сэмплировании Томпсона и нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space_bandits import NeuralBandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 3 \n",
    "num_features = 2\n",
    "model = NeuralBandits(num_actions, num_features, layer_sizes=[32,10], initial_pulls=100, initial_lr=0.01,verbose=False,show_training=False)\n",
    "X = []\n",
    "y = []\n",
    "a = []\n",
    "total_regret = 0\n",
    "cumulative_reward = 0\n",
    "cumulative_regret_list= []\n",
    "cumulative_reward_list = []\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(desc = 'while loop', total = n)\n",
    "while i != n:\n",
    "    \n",
    "    #generate a customer\n",
    "    cust = get_customer()\n",
    "    reward_vec = get_rewards(cust)\n",
    "    #prepare features for model\n",
    "    context = np.array([cust[1]])\n",
    "        \n",
    "                    \n",
    "        # Display the selected arm\n",
    "    if (i+1) < 500:\n",
    "        #arm = model.action(context)\n",
    "        arm = np.random.choice(arm_indices)\n",
    "    else:\n",
    "        arm = model.action(context)\n",
    "    i += 1\n",
    "    \n",
    "    reward = reward_vec[arm]\n",
    "    X.append(context.tolist()[0])\n",
    "    y.append(reward)\n",
    "    a.append(arm)\n",
    "    \n",
    "    best_choice = optimal_choices[cust[0]]\n",
    "    mx = reward_vec[best_choice]\n",
    "    regret = mx - reward\n",
    "    total_regret += regret\n",
    "    cumulative_regret_list.append(total_regret)\n",
    "    cumulative_reward += reward\n",
    "    cumulative_reward_list.append(cumulative_reward)\n",
    "    pbar.update(1)\n",
    "    #Update the models with the latest batch of data\n",
    "    if i  % 500 == 0 and i  != n :\n",
    "        print(\"Updating the models at i:\", i )\n",
    "        \n",
    "        train_df = pd.DataFrame(X,\n",
    "                            columns=[\"Age\", 'ARPU'])\n",
    "        train_df['reward'] = y\n",
    "        train_df['arm'] = a\n",
    "        print('Распределение ручек:')\n",
    "        print(train_df.arm.value_counts())\n",
    "        print(f\"Текущий regret: {cumulative_regret_list[-1]}\")\n",
    "        \n",
    "        for index, row in tqdm(train_df.iterrows(),total =train_df.shape[0]):\n",
    "            context = row[[\"Age\", 'ARPU']].values\n",
    "            action = int(row['arm'])\n",
    "            reward = float(row['reward'])\n",
    "            model.update(context, action, reward)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        a = []\n",
    "            \n",
    "pbar.close()\n",
    "df_cmab_regrets['SB_neural_linear'] = cumulative_regret_list\n",
    "df_cmab_rewards['SB_neural_linear'] = cumulative_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYMC linear CMAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся нашей реализацией линейного CMAB на сэмплировании Томпсона "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функции для бандита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция отвечающая за инициализацию априорного распределения факторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_priors(arms, n_features):\n",
    "    posteriors = {}\n",
    "    for arm in arms:\n",
    "        m = np.zeros(n_features)\n",
    "        s = np.ones(n_features)\n",
    "        posteriors[arm] = [m, s]\n",
    "    return posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для байесовской линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bandit_model(data, priors):\n",
    "    ba_model = bmb.Model(\"reward ~ Age + ARPU\", \n",
    "              data = data, \n",
    "              family=\"gaussian\",\n",
    "              priors={'Intercept':  bmb.Prior(\"Normal\",mu= priors[0][0], sigma = priors[1][0]),\n",
    "                            'Age': bmb.Prior(\"Normal\",mu= priors[0][1], sigma = priors[1][1]),\n",
    "                            'ARPU': bmb.Prior(\"Normal\",mu= priors[0][2], sigma = priors[1][2]),\n",
    "                            'reward_sigma':  bmb.Prior(\"Normal\",mu= priors[0][3], sigma = priors[1][3])})\n",
    "    trace = ba_model.fit(draws=2000,init='advi', target_accept=0.85, random_seed=42,inference_method='nuts_numpyro') #inference_method='nuts_numpyro'\n",
    "    return trace  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для семплирования Томпсона "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_arm_thompson(posteriors, context, arms, alpha = 1):\n",
    "    samples = {}\n",
    "    context = np.insert(context, 0, 1)\n",
    "    for arm in arms:\n",
    "        m = posteriors[arm][0][:-1]\n",
    "        s = posteriors[arm][1][:-1]* alpha\n",
    "        w = np.random.normal(m, s)\n",
    "        sample_prediction = np.dot(w, context)\n",
    "        samples[arm] = sample_prediction\n",
    "        \n",
    "    max_value = max(samples.values()); \n",
    "    max_keys = [key for key, value in samples.items() if value == max_value]\n",
    "    return np.random.choice(max_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим симуляцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_regret_list= []\n",
    "cumulative_reward_list = []\n",
    "total_regret = 0\n",
    "cumulative_reward = 0\n",
    "n_arms = 3\n",
    "n_features = 4\n",
    "arm_indices = np.array(range(n_arms))\n",
    "posteriors = get_priors(arm_indices, n_features)\n",
    "i = 0\n",
    "X = {arm: [] for arm in arm_indices}\n",
    "y = {arm: [] for arm in arm_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(desc = 'while loop', total = n)\n",
    "while i != n:\n",
    "    \n",
    "    #generate a customer\n",
    "    cust = get_customer()\n",
    "    reward_vec = get_rewards(cust)\n",
    "    #prepare features for model\n",
    "    context = np.array([cust[1]])\n",
    "        \n",
    "                    \n",
    "        # Display the selected arm\n",
    "    if (i+1) < 500:\n",
    "        arm = select_arm_thompson(posteriors, context, arm_indices,alpha=0.25)\n",
    "    else:\n",
    "        arm = select_arm_thompson(posteriors, context, arm_indices,alpha=0.25)\n",
    "    i += 1\n",
    "    \n",
    "    reward = reward_vec[arm]\n",
    "    X[arm].append(context.tolist()[0])\n",
    "    y[arm].append(reward)\n",
    "    \n",
    "    best_choice = optimal_choices[cust[0]]\n",
    "    mx = reward_vec[best_choice]\n",
    "    \n",
    "    regret = mx - reward\n",
    "    total_regret += regret\n",
    "    cumulative_regret_list.append(total_regret)\n",
    "    cumulative_reward += reward\n",
    "    cumulative_reward_list.append(cumulative_reward)\n",
    "    pbar.update(1)\n",
    "    #Update the models with the latest batch of data\n",
    "    if (i + 1) % 500 == 0 and (i + 1) != n :\n",
    "        print(\"Updating the models at i:\", i + 1)\n",
    "        print(f\"Текущий regret: {cumulative_regret_list[-1]}\")\n",
    "\n",
    "        priors = posteriors\n",
    "        posteriors = {}\n",
    "        for arm in arm_indices:\n",
    "            if X[arm] == [] or len(X[arm]) < 20:\n",
    "                posteriors[arm] = priors[arm]\n",
    "            else:\n",
    "                \n",
    "                train_df = pd.DataFrame(X[arm],\n",
    "                            columns=[\"Age\", 'ARPU'])\n",
    "                train_df['reward'] = y[arm]\n",
    "            \n",
    "                trace = get_bandit_model(train_df, priors[arm])\n",
    "            \n",
    "                m = az.summary(trace)[['mean','sd']].values[:,0]\n",
    "                s = az.summary(trace)[['mean','sd']].values[:,1] \n",
    "                                                \n",
    "                posteriors[arm] = [m, s]\n",
    "        X = {arm: [] for arm in arm_indices}\n",
    "        y = {arm: [] for arm in arm_indices}\n",
    "            \n",
    "pbar.close()\n",
    "df_cmab_regrets['PyMC_linear'] = cumulative_regret_list\n",
    "df_cmab_rewards['PyMC_linear'] = cumulative_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYMC neural CMAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся нашей реализацией нелинейного CMAB на сэмплировании Томпсона и нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функции для бандита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим нейронную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(in_features = 5, out_features=32)\n",
    "        self.layer_2 = nn.Linear(in_features = 32, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features = 10, out_features=3)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.layer_1(x))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_3(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "                \n",
    "    \n",
    "    def get_representation(self, contexts):\n",
    "        with torch.no_grad():\n",
    "            x = self.relu(self.layer_1(contexts))\n",
    "            x = self.relu(self.layer_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_action_mask(actions, num_actions):\n",
    "    ohe = torch.zeros((len(actions), 3))\n",
    "    actions = actions.reshape(-1, 1)\n",
    "    return ohe.scatter_(1, actions, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target(rewards, actions, num_actions):\n",
    "    \n",
    "    ohe = torch.zeros((len(actions), num_actions))\n",
    "    actions = actions.reshape(-1, 1)\n",
    "    rewards = rewards.reshape(-1, 1)\n",
    "    return ohe.scatter_(1, actions, rewards.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим функцию для нашего бандита с фичами из нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bandit_model(data, priors):\n",
    "    ba_model = bmb.Model(\"reward ~ 0 + z1 + z2 + z3 + z4 +z5 + z6 + z7 + z8 + z9 + z10\",\n",
    "              data = data, \n",
    "              family=\"gaussian\",\n",
    "              priors={      'z1': bmb.Prior(\"Normal\",mu= priors[0][0], sigma = priors[1][0]),\n",
    "                            'z2': bmb.Prior(\"Normal\",mu= priors[0][1] , sigma = priors[1][1]),\n",
    "                            'z3': bmb.Prior(\"Normal\",mu= priors[0][2], sigma = priors[1][2]),\n",
    "                            'z4': bmb.Prior(\"Normal\",mu= priors[0][3], sigma = priors[1][3]),\n",
    "                            'z5': bmb.Prior(\"Normal\",mu= priors[0][4], sigma = priors[1][4]),\n",
    "                              'z6': bmb.Prior(\"Normal\",mu= priors[0][5], sigma = priors[1][5]),\n",
    "                              'z7': bmb.Prior(\"Normal\",mu= priors[0][6], sigma = priors[1][6]),\n",
    "                              'z8': bmb.Prior(\"Normal\",mu= priors[0][7], sigma = priors[1][7]),\n",
    "                              'z9': bmb.Prior(\"Normal\",mu= priors[0][8], sigma = priors[1][8]),\n",
    "                              'z10': bmb.Prior(\"Normal\",mu= priors[0][9], sigma = priors[1][9]),\n",
    "                            'reward_sigma':  bmb.Prior(\"Normal\",mu= priors[0][10], sigma = priors[1][10])})\n",
    "    trace = ba_model.fit(draws=3000,init='advi', target_accept=0.85, random_seed=42,inference_method='nuts_numpyro')\n",
    "    return trace  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для семплирования Томпсона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_arm_thompson(model, posteriors, context, arms, alpha = 0.25):\n",
    "    samples = {}\n",
    "    for arm in arms:\n",
    "        act = build_action_mask(torch.tensor([arm]), len(arms))\n",
    "        context_nn = torch.cat((torch.tensor(context), act),1) \n",
    "        z = model.get_representation(context_nn.float()).detach().numpy()[0]\n",
    "        m = posteriors[arm][0][:-1]\n",
    "        s = posteriors[arm][1][:-1]* alpha\n",
    "        w = np.random.normal(m, s)\n",
    "        sample_prediction = np.dot(w, z)\n",
    "        samples[arm] = sample_prediction\n",
    "        \n",
    "    max_value = max(samples.values()); \n",
    "    max_keys = [key for key, value in samples.items() if value == max_value]\n",
    "    return np.random.choice(max_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса для нейронки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.uniform_(m.weight, a=-0.3, b=0.3)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим симуляцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_regret_list= []\n",
    "cumulative_reward_list = []\n",
    "total_regret = 0\n",
    "cumulative_reward = 0\n",
    "n_arms = 3\n",
    "n_features = 11\n",
    "epochs = 100\n",
    "nn_model =  NN_model().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(nn_model.parameters(), lr=0.01)\n",
    "scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler2 = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "nn_model = nn_model.apply(init_weights)\n",
    "\n",
    "\n",
    "arm_indices = np.array(range(n_arms))\n",
    "posteriors = get_priors(arm_indices, n_features)\n",
    "i = 0\n",
    "X = []\n",
    "y = []\n",
    "a = []\n",
    "i = 0\n",
    "features = ['z'+str(i) for i in np.arange(1,n_features)]\n",
    "noise = np.random.normal(0, 0.0001, 500) # Случайный шум для обхода ошибки с константой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(desc = 'while loop', total = n)\n",
    "while i != n:\n",
    "    \n",
    "    #generate a customer\n",
    "    cust = get_customer()\n",
    "    reward_vec = get_rewards(cust)\n",
    "    #prepare features for model\n",
    "    context = np.array([cust[1]])\n",
    "        \n",
    "                    \n",
    "        # Display the selected arm\n",
    "    if (i) < 500:\n",
    "        arm = np.random.choice(arm_indices)\n",
    "    else:\n",
    "        arm = select_arm_thompson(nn_model, posteriors, context, arm_indices)\n",
    "    i += 1\n",
    "    \n",
    "    reward = reward_vec[arm]\n",
    "    X.append(context.tolist()[0])\n",
    "    y.append(reward)\n",
    "    a.append(arm)\n",
    "    \n",
    "    best_choice = optimal_choices[cust[0]]\n",
    "    mx = reward_vec[best_choice]\n",
    "    regret = mx - reward\n",
    "    total_regret += regret\n",
    "    cumulative_regret_list.append(total_regret)\n",
    "    cumulative_reward += reward\n",
    "    cumulative_reward_list.append(cumulative_reward)\n",
    "    pbar.update(1)\n",
    "    #Update the models with the latest batch of data\n",
    "    if i % 500 == 0 and  i != n:\n",
    "        print(\"Updating the models at i:\", i )\n",
    "        \n",
    "        batch_df = pd.DataFrame(X,\n",
    "                            columns=[\"Age\", 'ARPU'])\n",
    "        batch_df['reward'] = y\n",
    "        batch_df['actions'] = a\n",
    "        print(batch_df.actions.value_counts())\n",
    "        print(cumulative_regret_list[-1])\n",
    "\n",
    "    \n",
    "        w_batch = build_action_mask(torch.tensor(batch_df[['actions']].values), 3)\n",
    "        X_batch = torch.cat((torch.tensor(batch_df[[\"Age\", 'ARPU']].values), w_batch),1) \n",
    "        y_batch = build_target(torch.tensor(batch_df['reward'].values), torch.tensor(batch_df[['actions']].values), 3)\n",
    "        print('Train Skynet')\n",
    "        X_batch, y_batch, w_batch = X_batch.to(device), y_batch.to(device), w_batch.to(device)\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            \n",
    "            for indx in np.arange(0,501,50):\n",
    "                \n",
    "                # 1. Forward pass\n",
    "                y_pred = nn_model(X_batch[indx:indx+50].float())\n",
    "                y_pred *= w_batch[indx:indx+50]\n",
    "\n",
    "                # 2. Calculate loss and accuracy\n",
    "                loss = loss_fn(y_pred, y_batch[indx:indx+50].float()) \n",
    "\n",
    "\n",
    "                # 3. Optimizer zero grad\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 4. Loss backward\n",
    "                loss.backward()\n",
    "\n",
    "                # 5. Optimizer step\n",
    "                optimizer.step()\n",
    "            scheduler1.step()\n",
    "            scheduler2.step()\n",
    "\n",
    "               \n",
    "        \n",
    "        z_values = nn_model.get_representation(X_batch.float())\n",
    "        z_values = pd.DataFrame(z_values.detach().numpy())\n",
    "        z_values.columns = features\n",
    "        batch_df = pd.concat([batch_df, z_values],axis=1)\n",
    "        batch_df[features] = batch_df[features].apply(lambda x: x + noise)\n",
    "        \n",
    "        print('Update bandit')\n",
    "        \n",
    "        priors = posteriors\n",
    "        posteriors = {}\n",
    "        for arm in arm_indices:\n",
    "            if (batch_df.loc[batch_df['actions'] ==arm].empty) == True:\n",
    "                posteriors[arm] = priors[arm]\n",
    "            else:\n",
    "                \n",
    "                trace = get_bandit_model(batch_df.loc[batch_df['actions'] ==arm][['reward']+features], priors[arm])\n",
    "            \n",
    "                m = az.summary(trace)[['mean','sd']].values[:,0]\n",
    "                s = az.summary(trace)[['mean','sd']].values[:,1]\n",
    "                                                \n",
    "                posteriors[arm] = [m, s]\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        a = []\n",
    "            \n",
    "pbar.close()\n",
    "df_cmab_regrets['PyMC_neural_linear'] = cumulative_regret_list\n",
    "df_cmab_rewards['PyMC_neural_linear'] = cumulative_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на бенчмарк "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_cmab_rewards,labels={\n",
    "                     \"value\": \"Reward\",\n",
    "                    \"index\":'views',\n",
    "                    \"variable\": \"Approach\"},)\n",
    "\n",
    "fig.update_layout(title_text=f\"\"\"Кумулятивный reward\"\"\" ,showlegend=True)\n",
    "\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_cmab_regrets,labels={\n",
    "                     \"value\": \"Regret\",\n",
    "                    \"index\":'views',\n",
    "                    \"variable\": \"Approach\"},)\n",
    "\n",
    "fig.update_layout(title_text=f\"\"\"Кумулятивный regret\"\"\" ,showlegend=True)\n",
    "\n",
    "fig.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
